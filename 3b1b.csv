https://www.youtube.com/watch?v=9vKqVkMQHKk 3Blue1Brown video   The goal here is simple: Explain what a derivative is. Thing is, though, there’s some subtlely to this topic, and some potential for paradoxes if you’re not careful, so the secondary goal is that you have some appreciation for what those paradoxes are and how to avoid them. You see, it’s common for people to say that the derivative measures “instantaneous rate of change”, but if you think about it, that phrase is actually an oxymoron: Change is something that happens between separate points in time, and when you blind yourself to all but a single instant, there is no more room for change. You’ll see what I mean as we get into it, and when you appreciate that a phrase like “instantaneous rate of change” is nonsensical, it makes you appreciate how clever the fathers of calculus were in capturing the idea this phrase is meant to evoke with a perfectly sensible piece of math: The derivative. As our central example, imagine a car that starts at some point A, speeds up, then slows to a stop at some point B 100 meters away, all over the course of 10 seconds. This is the setup I want you to keep in mind while I lay out what exactly a derivative is. We could graph this motion, letting a vertical axis represent the distance traveled, and a horizontal axis represent time. At each time t, represented with a point on the horizontal axis, the height of the graph tells us how far the car has traveled after that amount of time. It’s common to name a distance function like this s(t). I’d use the letter d for distance, except that it already has another full time job in calculus. Initially this curve is quite shallow, since the car is slow at the start. During the first second, the distance traveled by the car hardly changes at all. For the next few seconds, as the car speeds up, the distance traveled in a given second gets larger, corresponding to a steeper slope in the graph. And as it slows towards the end, the curve shallows out again. If we were to plot the car’s velocity in meters per second as a function of time, it might look like this bump. At time t=0, the velocity is 0. Up to the middle of the journey, the car builds up to some maximum velocity, covering a relatively large distance in each second. Then it slows back down to a speed of 0 meters per second. These two curves are highly related to each other; if you change the specific distance vs. time function, you’ll have some different velocity vs. time function. We want to understand the specifics of this relationship. Exactly how does velocity depend on this distance vs. time function. It’s worth taking a moment to think critically about what velocity actually means here. Intuitively, we all know what velocity at a given moment means, it’s whatever the car’s speedometer shows in that moment. And intuitively, it might make sense that velocity should be higher at times when the distance function is steeper; when the car traverses more distance per unit time. But the funny thing is, velocity at a single moment makes no sense. If I show you a picture of a car, a snapshot in an instant, and ask you how fast it’s going, you’d have no way of telling me. What you need are two points in time to compare, perhaps comparing the distance traveled after 4 seconds to the distance traveled after 5 second. That way, you can take the change in distance over the change in time. Right? That’s what velocity is, the distance traveled over a given amount of time. So how is it that we’re looking at a function for velocity that only takes in a single value for t, a single snapshot in time. It’s weird, isn’t it? We want to associate each individual point in time with a velocity, but computing velocity requires comparing two points in time. If that feels strange and paradoxical, good! You’re grappling with the same conflict that the fathers of calculus did, and if you want a deep understanding of rates of change, not just for a moving car, but for all sorts of scenarios in science, you’ll need a resolution to this apparent paradox. First let’s talk about the real world, then we’ll go into a purely mathematical one. Think about what an actual car’s speedometer might be doing. At some point, say 3 seconds into the journey, the speedometer might measure how far the car goes in a very small amount of time, perhaps the distance traveled between 3 seconds and 3.01 seconds. Then it would compute the speed in meters per second as that tiny distance, in meters, divided by that tiny time, 0.01 seconds. That is, a physical car can sidestep the paradox by not actually computing speed at a single point in time, and instead computing speed during very small amounts of time. Let’s call that difference in time “dt”, which you might think of as 0.01 seconds, and call the resulting difference in distance traveled “ds”. So the velocity at that point in time is ds over dt, the tiny change in distance over the tiny change in time. Graphically, imagine zooming in on the point of the distance vs. time graph above t=3. That dt is a small step to the right, since time is on the horizontal axis, and that ds is the resulting change in the height of the graph, since the vertical axis represents distance traveled. So ds/dt is the rise-over-run slope between two very close points on the graph. Of course, there’s nothing special about the value t=3, we could apply this to any other point in time, so we consider this expression ds/dt to be a function of t, something where I can give you some time t, and you can give back to me the value of this ratio at that time; the velocity as a function of time. So for example, when I had the computer draw this bump curve here representing the velocity function, the one you can think of as the slope of this distance vs. time function at each point, here’s what I had computer do: First, I chose some small value for dt, like 0.01. Then, I had the computer look at many times t between 0 and 10, and compute the distance function s at (t + dt), minus the value of this function at t. That is, the difference in the distance traveled between the given time t, and the time 0.01 seconds after that. Then divide that difference by the change in time dt, and this gives the velocity in meters per second around each point in time. With this formula, you can give the computer any curve representing the distance function s(t), and it can figure out the curve representing the velocity v(t). So now would be a good time to pause, reflect, make sure this idea of relating distance to velocity by looking at tiny changes in time dt makes sense, because now we’re going to tackle the paradox of the derivative head-on. This idea of ds/dt, a tiny change in the value of the function s divided by a tiny change in the input t, is almost what the derivative is. Even though out car’s speedometer will look at an actual change in time like 0.01 seconds to compute speed, and even though my program here for finding a velocity function given a position function also uses a concrete value of dt, in pure math, the derivative is not this ratio ds/dt for any specific choice of dt. It is whatever value that ratio approaches as the choice for dt approaches 0 Visually, asking what this ratio approaches has really a nice meaning: For any specific choice of dt, this ratio ds/dt is the slope of a line passing through two points on the graph, right? Well, as dt approaches 0, and those two points approach each other, the slope of that line approaches the slope of a line tangent to the graph at whatever point t we’re looking at. So the true, honest to goodness derivative, is not the rise-over-run slope between two nearby points on the graph; it’s equal to the slope of a line tangent to the graph at a single point. Notice what I’m not saying: I’m not saying that the derivative is whatever happens when dt is infinitely small, nor am I saying that you plug in 0 for dt. This dt is always a finitely small, nonzero value, it’s just approaching 0 is all. So even though change in an instant makes no sense, this idea of letting dt approach 0 is a really clever backdoor way to talk reasonably about the rate of change at a single point in time. Isn’t that neat? It’s flirting with the paradox of change in an instant without ever needing to touch it. And it comes with such a nice visual intuition as the slope of a tangent line at a single point on this graph. Since change in an instant still makes no sense, I think it’s healthiest for you to think of this slope not as some “instantaneous rate of change”, but as the best constant approximation for rate of change around a point. It’s worth saying a few words on notation here. Throughout this video I’ve been using “dt” to refer to a tiny change in t with some actual size, and “ds” to refer to the resulting tiny change in s, which again has an actual size. This is because that’s how I want you to think about them. But the convention in calculus is that whenever you’re using the letter “d” like this, you’re announcing that the intention is to eventually see what happens as dt approaches 0. For example, the honest-to-goodness derivative of the function s(t) is written as ds/dt, even though the derivative is not a fraction, per se, but whatever that fraction approaches for smaller and smaller nudges in t. A specific example should help here. You might think that asking about what this ratio approaches for smaller and smaller values of dt would make it much more difficult to compute, but strangely it actually makes things easier. Let’s say a given distance vs. time function was exactly t3. So after 1 second, the car has traveled 13=1 meters, after 2 seconds, it’s traveled 23=8 meters, and so on. What I’m about to do might seem somewhat complicated, but once the dust settles it really is simpler, and it’s the kind of thing you only ever have to do once in calculus. Let’s say you want the velocity, ds/dt, at a specific time, like t=2. And for now, think of dt having an actual size; we’ll let it go to 0 in just a bit. The tiny change in distance between 2 seconds and 2+dt seconds is s(2+dt)-s(2), and we divide by dt. Since s(t) = t3, that numerator is (2+dt)3 - 23. Now this, we can work out algebraically. And again bear with me, there’s a reason I’m showing you the details. Expanding the top gives 23 + 3*22dt + 3*2*(dt)2 + (dt)3 - 23. There are several terms here, and I want you to remember that it looks like a mess, but it simplifies. Those 23 terms cancel out. Everything remaining has a dt, so we can divide that out. So the ratio ds/dt has boiled down to 3*22 + two different terms that each have a dt in them. So as dt approaches 0, representing the idea of looking at smaller and smaller changes in time, we can ignore those! By eliminating the need to think of a specific dt, we’ve eliminated much of the complication in this expression! So what we’re left with is a nice clean 3*22. This means the slope of a line tangent to the point at t=2 on the graph of t3 is exactly 3*22, or 12. Of course, there was nothing special about choosing t=2; more generally we’d say the derivative of t3, as a function of t, is 3*t2. That’s beautiful. This derivative is a crazy complicated idea: We’ve got tiny changes in distance over tiny changes in time, but instead of looking at any specific tiny change in time we start talking about what this thing approaches. I mean, it’s a lot to think about. Yet we’ve come out with such a simple expression: 3t2. In practice, you would not go through all that algebra each time. Knowing that the derivative t3 is 3t2 is one of those things all calculus students learn to do immediately without rederiving each time. And in the next video, I’ll show ways to think about this and many other derivative formulas in nice geometric ways. The point I want to make by showing you the guts here is that when you consider the change in distance of a change in time for any specific value of dt, you’d have a whole mess of algebra riding along. But by considering what this ratio approaches as dt approaches 0, it lets you ignore much of that mess, and actually simplifies the problem. Another reason I wanted to show you a concrete derivative like this is that it gives a good example for the kind of paradox that come about when you believe in the illusion of an instantaneous rate of change. Think about this car traveling according to this t3 distance function, and consider its motion at moment t=0. Now ask yourself whether or not the car is moving at that time. On the one hand, we can compute its speed at that point using the derivative of this function, 3t2, which is 0 at time t=0. Visually, this means the tangent line to the graph at that point is perfectly flat, so the car’s quote unquote “instantaneous velocity” is 0, which suggests it’s not moving. But on the other hand, if it doesn’t start moving at time 0, when does it start moving? Really, pause and ponder this for a moment, is that car moving at t=0? Do you see the paradox? The issue is that the question makes no sense, it references the idea of of change in a moment, which doesn’t exist. And that’s just not what the derivative measures. What it means for the derivative of the distance function to be 0 at this point is that the best constant approximation for the car’s velocity around that point is 0 meters per second. For example, between t=0 and t=0.1 seconds, the car does move... it moves 0.001 meters. That’s very small, and importantly it’s very small compared to the change in time, an average speed of only 0.01 meters per second. What it means for the derivative of this motion to be 0 is that for smaller and smaller nudges in time, this ratio of change in distance over change in time approaches 0, though in this case it never actually hits it. But that’s not to say the car is static. Approximating its movement with a constant velocity of 0, after all, just an approximation. So if you ever hear someone refer to the derivative as an “instantaneous rate of change”, a phrase which is intrinsically oxymoronic, think of it as a conceptual shorthand for “the best constant approximation for the rate of change” In the following videos I’ll talk more about the derivative; what does it look like in different contexts, how do you actually compute it, what’s it useful for, things like that.
https://www.youtube.com/watch?v=S0_qX4VJhMQ 3Blue1Brown video   Now that we've seen what a derivative means, and what it has to do with rates of change. Our next step is to learn how to actually compute these guys, as in if I give you some kind of function with an explicit formula you'd want to be able to find what the formula for its derivative is. Maybe it's obvious, but I think it's worth stating explicitly why this is an important thing to be able to do. Why much of a calculus students time ends up going towards grappling with derivatives of abstract functions rather than thinking about concrete rate of change problems, Is because a lot of real-world phenomena. The sort of things that we want to use calculus to analyze are modeled using polynomials, trigonometric functions, exponential's and other pure functions like that. So if you build up some fluency with the ideas of rates of change for those kinds of pure abstract functions. It gives you a language to more readily talk about the rates at which things change in concrete situations that you might be using calculus to model. But it is way too easy for this process to feel like just memorizing a list of rules. And if that happens, if you get that feeling it's all so easy to lose sight of the fact that derivatives are fundamentally about just looking at tiny changes to some quantity, and how that relates to a resulting tiny change in another quantity. So in this video and in the next one, my aim is to show you how you can think about a few of these rules intuitively and geometrically. And I really want to encourage you to never forget that tiny nudges are at the heart of derivatives. let's start with a simple function like f(x) = x^2, what if I asked you it's derivative. That is if you were to look at some value x like x = 2 and compare it to a value slightly bigger, just dx bigger. What's the corresponding change in the value of the function df, and in particular what's df divided by dx - The rate at which this function is changing per unit change in x as a first step for intuition. We know that you can think of this ratio df/dx as the slope of a tangent line to the graph of x^2, and from that you can see that the slope generally increases as x increases. At 0 the tangent line is flat so the slope is 0 at x = 1. That's something a bit steeper at x = 2, it's steeper still but looking at graphs isn't generally the best way to understand the precise formula for a derivative, for that it's best to take a more literal look at what x^2 actually means. And in this case let's go ahead and picture a square whose side length is x if you increase x by some tiny nudge, some little dx. What's the resulting change in the area of that square, that slight change, in area is what df means in this context. It's the tiny increase to the value of f(x) = x^2 caused by increasing x by that tiny nudge dx. Now you can see that there's three new bits of area in this diagram, two thin rectangles and a miniscule square. The two thin rectangles each have side lengths of x and dx so they account for two times x * dx units of new area. For example let's say x was 3 and dx was 0.01. Then that new area from these two thin rectangles would be 2 * 3 * 0.01 which is 0.06, about 6 times the size of dx. That little square there has an area of dx^2, but you should think of that as being really tiny, negligibly tiny. For example, if dx was 0.01, that would be only 0.0001. And keep in mind, I'm drawing DX with a fair bit of width here just so we can actually see it, but always remember - in principle dx should be thought of as a truly tiny amount and for those truly tiny amounts. A good rule of thumb is that you can ignore anything that includes a dx raised to a power greater than 1. That is a tiny change squared is a negligible change what this leaves us with is that df is just some multiple of dx, and that multiple 2x which you could also write as df/dx is the derivative of x^2. For example, if you were starting at x = 3, then as you slightly increase x the rate of change in the area per unit change in length added dx^2/dx would be 2 * 3 or 6. And if instead you were starting at x = 5, the rate of change would be ten units of area per unit change in x. Let's go ahead and try a different simple function f(x) = x^3. This is going to be the geometric view of the stuff that I went through algebraically in the last video. What's nice here is that we can think of x^3 as the volume of an actual cube, whose side lengths are x. And when you increase x by a tiny nudge, a tiny dx the resulting increase in volume is what I have here in yellow. That represents all the volume in a cube with side lengths x plus dx. That's not already in the original cube, the one with side length x. It's nice to think of this new volume as broken up into multiple components, but almost all of it comes from these three square faces, or set a little more precisely as dx approaches zero. Precisely as dx approaches zero, those three squares comprise a portion closer and closer to 100% of that new yellow volume. Each of those thin squares has a volume of x^2 * dx, the area of the face times that little thickness dx. So in total this gives us 3x^2 dx of volume change. And to be sure there are other slivers of volume here, along the edges, and that tiny one in the corner. But all of that volume is going to be proportional to dx^2 or dx^3 so we can safely ignore them. Again this is ultimately because they're going to be divided by dx, and if there's still any dx remaining then those terms aren't going to survive the process of letting dx approach 0. What this means is that the derivative of x^3, the rate at which x^3 changes per unit change of x is 3x^2. What that means in terms of graphical intuition is that the slope of the graph of x^3 at every single point x is exactly 3x^2. And reasoning about that slope, it should make sense that this derivative is high on the left, and then zero at the origin, and then high again as you move to the right. But just thinking in terms of the graph would never have landed us on the precise quantity 3x^2. For that we had to take a much more direct look at what x^3 actually means now in practice. You wouldn't necessarily think of the square every time you're taking the derivative of x^2. Nor would you necessarily think of this cube whenever you're taking the derivative of x^3, both of them fall under a pretty recognizable pattern for polynomial terms the derivative of x^4 turns out to be 4x^3. The derivative of x^5 is 5x^4. The derivative of x to the n for any power n is nx^(n-1). This right here is what's known in the business as the power rule. In practice we all quickly just get jaded and think about this symbolically as the exponent hopping down in front leaving behind one less than itself. Rarely pausing to think about the geometric delights that underlie these derivatives. That's the kind of thing that happens when these tend to fall in the middle of much longer computations. But rather than tracking it all off to symbolic patterns let's just take a moment and think about why this works. For powers beyond just 2 and 3 when you nudge that input x, increasing it slightly to x + dx, working out the exact value of that nudged output would involve multiplying together these n separate x + dx terms. The full expansion would be really complicated but part of the point of derivatives is that most of that complication can be ignored. The first term in your expansion is x^n, this is analogous to the area of the original square or the volume of the original cube from our previous examples. For the next terms in the expansion, you can choose mostly x's with a single dx since there are n radicals from which you could have chosen that single dx. This gives us n separate terms all of which include n-1 x's times a dx giving a value of x^(n-1) times dx. This is analogous to how the majority of the new area in the square came from those two bars each with area x * dx, or how the bulk of the new volume in the cube came from those three thin squares. Each of which had a volume of x^2 times dx. There will be many other terms of this expansion but all of them are just going to be some multiple of dx^2 so we can safely ignore them. And what that means is that all but a negligible portion of the increase in the output comes from n copies of this x to the (n-1) * dx - that's what it means. For the derivative of x^n to be n * x^(n-1) and even though, like I said in practice, you'll find yourself performing this derivative quickly and symbolically imagining the exponent, hopping down to the front. Every now and then it's nice to just step back and remember why these rules work. Not just because it's pretty, and not just because it helps remind us that math actually makes sense and isn't just a pile of formulas to memorize. But because it flexes that very important muscle of thinking about derivatives in terms of tiny nudges. As another example think of the function f(x) = 1/x. Now, on the one hand you could just blindly try applying the power rule since 1/x is the same as writing x^(-1). That would involve letting the negative 1 hop down in front leaving behind 1 less than itself which is -2. But let's have some fun and see if we can reason about this geometrically rather than just plugging it through some formula. The value 1/x is asking what number multiplied by x equals 1, so here's how I'd like to visualize it - imagine a little rectangular puddle of water sitting in two dimensions whose area is 1, and let's say that its width is x. Which means that the height has to be 1/x since the total area of it is 1, so if x was stretched out to 2 then that height is forced down to 1/2 and if you increased x up to 3 then the other side has to be squished down to 1/3 this is a nice way to think about the graph of 1 x. By the way, if you think of this with x of the puddle as being in the xy plane then that corresponding output 1/x, the height of the graph above that point is whatever the height of your puddle has to be to maintain an area of 1. So with this visual in mind for the derivative, imagine nudging up that value of x by some tiny amount, some tiny dx. How must the height of this rectangle change so that the area of the puddle remains constant at 1 - That is increasing the width by dx, add some new area to the right here so the puddle has to decrease in height by some d(1/x) so that the area lost off of that top cancels out the area gained. You should think of that d(1/x) as being a negative amount. By the way since it's decreasing the height of the rectangle and you know what I'm going to leave the last few steps here for you for you to pause and ponder and work out an ultimate expression and once you reason out what D of 1 over X / DX should be I want you to compare it to what you would have gotten if you had just blindly applied the power rule purely symbolically to X to the negative one and while I'm encouraging a deposit ponder here's another fun challenge if you're feeling up to it see if you can reason through what the derivative of the square root of x should be to finish things off I want to tackle one more type of function trigonometric functions and in particular let's focus on the sine function so for this section I'm going to assume that you're already familiar with how to think about trig functions using the unit circle the circle with the radius one centered at the origin for a given value of theta like say zero point eight you imagine yourself walking around the circle starting from the rightmost point until you've traversed that distance of zero point eight in arc length this is the same thing as saying that the angle right here is exactly theta radians since the circle has a radius of one then what sine of theta means is the height of that point above the x-axis and as your theta value increases and you walk around the circle your height Bob's up and down between negative one and one so when you graph sine of theta versus theta you get this wave pattern the quintessential wave pattern and just from looking at this graph we can start to get a feel for the shape of the derivative of the sine the slope at zero is something positive since sine of theta is increasing there and as we move to the right and sine of theta approaches its peak that slope goes down to zero then the slope is negative for a little while while the sine is decreasing before coming back up to zero as the sine graph levels out and as you continue thinking this through and drawing it out if you're familiar with the graph of trig functions you might guess that this derivative graph should be exactly cosine of theta since all the peaks and valleys line up perfectly with where the peaks and valleys for the cosine function should be and spoiler alert the derivative is in fact the cosine of theta but aren't you a little curious about why it's precisely cosine of theta I mean you could have all sorts of functions with peaks and valleys at the same points that have roughly the same shape but who knows maybe the derivative of sine could have turned out to be some entirely new type of function that just happens to have a similar shape well just like the previous examples a more exact understanding of the derivative requires looking at what the function actually represents rather than looking at the graph of the function so think back to that walk around the unit circle having traversed an arc with length theta and thinking about sine of theta as the height of that point now zoom in to that point on the circle and consider a slight nudge of D theta along their circumference a tiny step in your walk around the unit circle how much does that tiny step change the sign of theta how much does this increase D theta of arc length increase the height above the x-axis well zoomed in close enough the circle basically looks like a straight line in this neighborhood so let's go ahead and think of this right triangle where the hypotenuse of that right triangle represents the nudge D theta along the circumference and that left side here represents the change in height the resulting D sine of theta now this tiny triangle is actually similar to this larger triangle here with the defining angle theta and whose hypotenuse is the radius of the circle with length one specifically this little angle right here is precisely equal to theta radians now think about what the derivative of sine is supposed to mean it's the ratio between that D sine of theta the tiny change to the height divided by D theta the tiny change to the input of the function and from the picture we can see that that's the ratio between the length of the side adjacent to the angle theta divided by the hypotenuse well let's see adjacent divided by hypotenuse that's exactly what the cosine of theta means that's the definition of the cosine so this gives us two different really nice ways of thinking about how the derivative of sine is cosine one of them is looking at the graph and getting a loose feel for the shape of things based on thinking about the slope of the sine graph at every single point and the other is a more precise line of reasoning looking at the unit circle itself for those of you that like to pause and ponder see if you can try a similar line of reasoning to find what the derivative of the cosine of theta should be in the next video I'll talk about how you can take derivatives of functions who combine simple functions like these ones either as sums or products or function compositions things like that and similar to this video the goal is going to be to understand each one geometrically in a way that makes it intuitively reasonable and somewhat more memorable.
https://www.youtube.com/watch?v=YG15m2VwSjA 3Blue1Brown video   In the last videos I talked about the derivatives of simple functions, things like powers of x, sin(x), and exponentials, the goal being to have a clear picture or intuition to hold in your mind that explains where these formulas come from. Most functions you use to model the world involve mixing, combining and tweaking these these simple functions in some way; so our goal now is to understand how to take derivatives of more complicated combinations; where again, I want you to have a clear picture in mind for each rule. This really boils down into three basic ways to combine functions together: Adding them, multiplying them, and putting one inside the other; also known as composing them. Sure, you could say subtracting them, but that’s really just multiplying the second by -1, then adding. Likewise, dividing functions is really just the same as plugging one into the function 1/x, then multiplying. Most functions you come across just involve layering on these three types of combinations, with no bound on how monstrous things can become. But as long as you know how derivatives play with those three types of combinations, you can always just take it step by step and peal through the layers. So, the question is, if you know the derivatives of two functions, what is the derivative of their sum, of their product, and of the function compositions between them? The sum rule is the easiest, if somewhat tounge-twisting to say out loud: The derivative of a sum of two functions is the sum of their derivatives. But it’s worth warming up with this example by really thinking through what it means to take a derivative of a sum of two functions, since the derivative patterns for products and function composition won’t be so straight forward, and will require this kind of deeper thinking. The function f(x) = sin(x) + x2 is a function where, for every input, you add together the values of sin(x) and x2 at that point. For example, at x = 0.5, the height of the sine graph is given by this bar, the height of the x2 parabola is given by this bar, and their sum is the length you get by stacking them together. For the derivative, you ask what happens as you nudge the input slightly, maybe increasing it to 0.5+dx. The difference in the value of f between these two values is what we call df. Well, pictured like this, I think you’ll agree that the total change in height is whatever the change to the sine graph is, what we might call d(sin(x)), plus whatever the change to x2 is, d(x2). We know the derivative of sine is cosine, and what that means is that this little change d(sin(x)) would be about cos(x)dx. It’s proportional to the size of dx, with a proportionality constant equal to cosine of whatever input we started at. Similarly, because the derivative of x2 is 2x, the change in the height of the x2 graph is about 2x*dx. So, df/dx, the ratio of the tiny change to the sum function to the tiny change in x that caused it, is indeed cos(x)+2x, the sum of the derivatives of its parts. But like I said, things are a bit different for products. Let’s think through why, in terms of tiny nudges. In this case, I don’t think graphs are our best bet for visualizing things. Pretty commonly in math, all levels of math really, if you’re dealing with a product of two things, it helps to try to understand it as some form of area. In this case, you might try to configure some mental setup of a box whose side-lengths are sin(x) and x2. What would that mean? Well, since these are functions, you might think of these sides as adjustable; dependent on the value of x, which you might think of as a number that you can freely adjust. So, just getting the feel for this, focus on that top side, whose changes as the function sin(x). As you change the value of x up from 0, it increases in up to a length of 1 as sin(x) moves towards its peak. After that, it starts decreasing as sin(x) comes down from 1. And likewise, that height changes as x2. So f(x), defined as this product, is the area of this box. For the derivative, think about how a tiny change to x by dx influences this area; that resulting change in area is df. That nudge to x causes the width to change by some small d(sin(x)), and the height to change by some d(x2). This gives us three little snippets of new area: A thin rectangle on the bottom, whose area is its width, sin(x), times its thin height, d(x2); there’s a thin rectangle on the right, whose area is its height, x2, times its thin width, d(sin(x)). And there’s also bit in the corner. But we can ignore it, since its area will ultimately be proportional to dx2, which becomes negligible as dx goes to 0. This is very similar to what I showed last video, with the x2 diagram. Just like then, keep in mind that I’m using somewhat beefy changes to draw things, so we can see them, but in principle think of dx as very very small, meaning d(x2) and d(sin(x)) are also very very small. Applying what we know about the derivative of sine and x2, that tiny change d(x2) is 2x*dx, and that tiny change d(sin(x)) is cos(x)dx. Dividing out by that dx, the derivative df/dx is sin(x) by the derivative of x2, plus x2 by the derivative of sine. This line of reasoning works for any two functions. A common mnemonic for the product rule is to say in your head “left d right, right d left”. In this example, sin(x)*x2, “left d right” means you take the left function, in this case sin(x), times the derivative of the right, x2, which gives 2x. Then you add “right d left”: the right function, x2, times the derivative of the left, cos(x). Out of context, this feels like kind of a strange rule, but when you think of this adjustable box you can actually see how those terms represent slivers of area. “Left d right” is the area of this bottom rectangle, and “right d left” is the area of this rectangle on the right. By the way, I should mention that if you multiply by a constant, say 2*sin(x), things end up much simpler. The derivative is just that same constant times the derivative of the function, in this case 2*cos(x). I’ll leave it to you to pause and ponder to verify that this makes sense. Aside from addition and multiplication, the other common way to combine functions that comes up all the time is function composition. For example, let’s say we take the function x2, and shove it on inside sin(x) to get a new function, sin(x2). What’s the derivative of this new function? Here I’ll choose yet another way to visualize things, just to emphasize that in creative math, we have lots of options. I’ll put up three number lines. The top one will hold the value of x, the second one will represent the value of x2, and that third line will hold the value of sin(x2). That is, the function x2 gets you from line 1 to line 2, and the function sine gets you from line 2 to line 3. As I shift that value of x, maybe up to the value 3, then value on the second shifts to whatever x2 is, in this case 9. And that bottom value, being the sin(x2), will go over to whatever the sin(9) is. So for the derivative, let’s again think of nudging that x-value by some little dx, and I always think it’s helpful to think of x starting as some actual number, maybe 1.5. The resulting nudge to this second value, the change to x2 caused by such a dx, is what we might call d(x2). You can expand this as 2x*dx, which for our specific input that length would be 2*(1.5)*dx, but it helps to keep it written as d(x2) for now. In fact let me go one step further and give a new name to x2, maybe h, so this nudge d(x2) is just dh. Now think of that third value, which is pegged at sin(h). It’s change d(sin(h)); the tiny change caused by the nudge dh. By the way, the fact that it’s moving left while the dh bump is to the right just means that this change d(sin(h)) is some negative number. Because we know the derivative of sine, we can expand d(sin(h)) as cos(h)*dh; that’s what it means for the derivative of sine to be cosine. Unfolding things, replacing h with x2 again, that bottom nudge is cos(x2)d(x2). And we could unfold further, noting that d(x2) is 2x*dx. And it’s always good to remind yourself of what this all actually means. In this case where we started at x = 1.5 up top, this means that the size of that nudge on the third line is about cos(1.52)*2(1.5)*(the size of dx); proportional to the size of dx, where the derivative here gives us that proportionality constant. Notice what we have here, we have the derivative of the outside function, still taking in the unaltered inside function, and we multiply it by the derivative of the inside function. Again, there’s nothing special about sin(x) and x2. If you have two functions g(x) and h(x), the derivative of their composition function g(h(x)) is the derivative of g, evaluated at h(x), times the derivative of h. This is what we call the “chain rule”. Notice, for the derivative of g, I’m writing it as dg/dh instead of dg/dx. On the symbolic level, this serves as a reminder that you still plug in the inner function to this derivative. But it’s also an important reflection of what this derivative of the outer function actually represents. Remember, in our three-lines setup, when we took the derivative of sine on the bottom, we expanded the size of the nudge d(sin) as cos(h)*dh. This was because we didn’t immediately know how the size of that bottom nudge depended on x, that’s kind of the whole thing we’re trying to figure out, but we could take the derivative with respect to the intermediate variable h. That is, figure out how to express the size of that nudge as multiple of dh. Then it unfolded by figuring out what dh was. So in this chain rule expression, we’re saying look at the ratio between a tiny change in g, the final output, and a tiny change in h that caused it, h being the value that we’re plugging into g. Then multiply that by the tiny change in h divided by the tiny change in x that caused it. The dh’s cancel to give the ratio between a tiny change in the final output, and the tiny change to the input that, through a certain chain of events, brought it about. That cancellation of dh is more than just a notational trick, it’s a genuine reflection of the tiny nudges that underpin calculus. So those are the three basic tools in your belt to handle derivatives of functions that combine many smaller things: The sum rule, the product rule and the chain rule. I should say, there’s a big difference between knowing what the chain rule and product rules are, and being fluent with applying them in even the most hairy of situations. I said this at the start of the series, but it’s worth repeating: Watching videos, any videos, about these mechanics of calculus will never substitute for practicing them yourself, and building the muscles to do these computations yourself. I wish I could offer to do that for you, but I’m afraid the ball is in your court, my friend, to seek out practice. What I can offer, and what I hope I have offered, is to show you where these rules come from, to show that they’re not just something to be memorized and hammered away; but instead are natural patterns that you too could have discovered by just patiently thinking through what a derivative actually means.
https://www.youtube.com/watch?v=m2MIpDrF7Es 3Blue1Brown video   What's so special about Euler's number e? I've introduced a few derivative formulas but a really important one that Ieft out was exponentials. So here, I want to talk about the derivatives of functions like Two to the x, seven to the x, and also to show why e to the x is arguably the most important of the exponentials. First of all, to get an intuition, let's just focus on the function two to the x. And let's think of that input as a time, "t," maybe in days, and the output, 2 to the t, as a population size perhaps of a particularly fertile band of pi creatures which doubles every single day. And actually, instead of population size, which grows in discrete little jumps with each new baby pi creature, maybe let's think of 2 to the t as the total mass of the population. I think that better reflects the continuity of this function, don't you? So, for example, at time t=0, the total mass is 2 to the 0 equals 1, for the mass of one creature. At t=1 day, the population has grown to 2 to the 1 = 2 creature masses. At day t=2, it's t squared, or 4, and in general, it just keeps doubling every day. For the derivative, we want dm/dt, the rate at which this population mass is growing, thought of as a tiny change in the mass divided by a tiny change in time. And let's start by thinking of the rate of change over a full day, say, between day 3 and day 4. Well, in this case it grows from 8 to 16, so that's 8 new creature masses added over the course of one day. And notice, that rate of growth equals the population size at the start of the day. Between day 4 and day 5, it grows from 16 to 32. So that's a rate of 16 new creature masses per day. Which, again, equals the population size at the start of the day. And in general, this rate of growth over a full day equals the population size at the start of that day. So it might be tempting to say that this means the derivative of 2 to the t equals itself. That the rate of change of this function at a given time t, is equal to, well, the value of that function. And this is definitely in the right direction, but it's not quite correct. What we're doing here is making comparisons over a full day, considering the difference between 2 to the t plus 1, and 2 to the t. but for the derivative, we need to ask what happens for smaller and smaller changes. What's the growth over the course of a tenth of a day? A hundredth of a day? One one-billionth of a day? This is why I had us think of the function as representing population mass since it makes sense to ask about a tiny change in mass over a tiny fraction of a day but it doesn't make as much sense to ask about the tiny change in a discrete population size per second. More abstractly, for a tiny change in time, dt, we want to understand the difference between 2 to the t plus dt and 2 to the t, all divided by dt. A change in the function per unit time, but now we're looking very narrowly around a given point in time, rather than over the course of a full day. And here's the thing: I would love if there was some very clear geometric picture that made everything that's about to follow just pop out, some diagram where you could point to one value, and say, "See! *that* part. That is the derivative of 2 to the t." And if you know of one, please let me know. And while the goal here as with the rest of the series is to maintain a playful spirit of discover, the type of play that follows will have more to do with finding numerical patterns, rather than visual ones. So start by just taking a very close look at this term 2 to the t, plus dt A core property of exponentials is that you can break this up as 2 to the t times 2 to the dt. That really is the most important property of exponents. If you add two values in that exponent, you can break up the output as a product of some kind. This is what lets you relate additive ideas things like tiny steps in time, to multiplicative ideas, things like rates and ratios. I mean, just look at what happens here. After that move, we can factor out the term 2 to the t. which is now just multiplied by 2 to the dt minus 1, all divided by dt. And remember, the derivative of 2 to the t is whatever this whole expression approaches as dt approaches 0. And at first glance that might seem like an unimportant manipulation, but a tremendously important fact is that this term on the right, where all of the dt stuff lives, is completely separate from the t term itself. It doesn't depend on the actual time where we started. You can go off to a calculator and plug in very small values for dt here, for example, maybe typing in 2 to the 0.001 minus 1, divided by 0.001 What you'll find is that for smaller and smaller choices of dt, this value approaches a very specific number, around 0.6931. Don't worry if that number seems mysterious, The central point is that this is some kind of constant. Unlike derivatives of other functions, all of the stuff that depends on dt is separate from the value of t itself. So the derivative of 2 to the t is just itself, but multiplied by some constant And that should kind of make sense, because earlier, it felt like the derivative for 2 to the t should be itself, at least when we were looking at changes over the course of a full day. And evidently, the rate of change for this function over much smaller time scales is not quite equal to itself, but it's proportional to itself, with this very peculiar proportionality constant of 0.6931 And there's not too much special about the number 2 here, if instead we had dealt with the function 3 to the t, the exponential property would also have led us to the conclusion that the derivative of 3 to the t is proportional to itself. But this time it would have had a proportionality constant 1.0986. And for other bases to your exponent you can have fun trying to see what the various proportionality constants are, maying seeing if you can find a pattern in them. For example, if you plug in 8 to the power of a very tiny number minus 1, and divide by that same tiny number, what you'd find is that the relevant proportionality constant is around 2.079, and maybe, just maybe you would notice that this number happens to be exactly three times the constant associated with the base for 2, so these numbers certainly aren't random, there is some kind of pattern, but what is it? What does 2 have to do with the number 0.6931? And what does 8 have to do with the number 2.079? Well, a second question that is ultimately going to explain these mystery constants is whether there's some base where that proportionality constant is one (1), where the derivative of "a"to the power t is not just proportional to itself, but actually equal to itself. And there is! It's the special constant "e," around 2.71828. In fact, it's not just that the number e happens to show up here, this is, in a sense, what defines the number e. If you ask, "why does e, of all numbers, have this property?" It's a little like asking "why does pi, of all numbers happen to be the ratio of the circumference of a circle to its diameter?" This is, at its heart, what defines this value. All exponential functions are proportional to their own derivative, but e along is the special number so that that proportionality constant is one, meaning e to the t actually equals its own derivative. One way to think of that is that if you look at the graph of e to the t, it has the peculiar property that the slope of a tangent line to any point on this graph equals the height of that point above the horizontal axis. The existence of a function like this answers the question of the mystery constants and it's because it gives a different way to think about functions that are proportional to their own derivative. The key is to use the chain rule. For example, what is the derivative of e to the 3t? Well, you take the derivative of the outermost function, which due to this special nature of e is just itself and then multipliy it by the derivative of that inner function, 3t which is the constant, 3. Or, rather than just applying a rule blindly, you could take this moment to practice the intuition for the chain rule that I talked through last video, thinking about how a slight nudge to t changes the value of 3t and how that intermediate change nudges the final value of e to the 3t. Either way, the point is, e to the power of some constant times t is equal to that same constant times itself. And from here, the question of those mystery constants really just comes down to a certain algebraic manipulation. The number 2 can also be written as e to the natural log of 2. There's nothing fancy here, this is just the definition of the natural log, it asks the question, "e to what equals 2?" So, the function 2 to the t is the same as the function e to the power of the natural log of 2 times t. And from what we just saw, combining the facts that e to the t is its own derivative with the chain rule, the derivative of this function is proportional to itself, with a proportionality constant equal to the natural log of 2. And indeed, if you go plug in the natural log of two to a calculator, you'll find that it's 0.6931, the mystery constant that we ran into earlier. And the same goes for all of the other bases. The mystery proportionality constant that pops up when taking derivatives is just the natural log of the base, the answer to the question, "e to the what equals that base?" In fact, throughout applications of calculus, you rarely see exponentials written as some base to a power t, instead you almost always write the exponential as e to the power of some constant times t. It's all equivalent. I mean any function like 2 to the t or 3 to the t can also be written as e to some constant time t. At the risk of staying over-focused on the symbols here, Ireally want to emphasize that there are many many ways to write down any particular exponential function, and when you see something written as e to some constant time t, that's a choice that we make to write it that way, and the number e is not fundamental to that function itself. What is special about writing exponentials in terms of e like this, is that it gives that constant in the exponent a nice, readable meaning. Here, let me show you what I mean. All sorts of natural phenomena involve some rate of change that's proportional to the thing that's changing. For example, the rate of growth of a population actually does tend to be proportional to the size of the population itself, assuming there isn't some limited resource slowing things down. And if you put a cup of hot water in a cool room, the rate at which the water cools is proportional to the difference in temperature between the room and the water. Or, said a little differently the rate at which that difference changes is proportional to itself. If you invest your money, the rate at which it grows is proportional to the amount of money there at any time. In all of these cases, where some variable's rate of change is proportional to itself the function describing that variable over time is going to look like some kind of exponential. And even though there are lots of ways to write any exponential function, it's very natural to choose to express these functions as e to the power of some constant times t since that constant carries a very natural meaning. It's the same as the proportionality constant between the size of the changing variable and the rate of change. And, as always, I want to thank those who have made this series possible.
https://www.youtube.com/watch?v=qb40J4N1fa4 3Blue1Brown video   Let me share with you something I found particularly weird when I was a student first learning calculus. Let’s say you have a circle with radius 5 centered at the origin of the xy-coordinate plane, which is defined using the equation x^2 + y^2 = 5^2. That is, all points on this circle are a distance 5 from the origin, as encapsulated by the pythagorean theorem with the sum of the squares of the legs of this triangle equalling the square of the hypotenuse, 52. And suppose you want to find the slope of a tangent line to this circle, maybe at the point (x, y) = (3, 4). Now, if you’re savvy with geometry, you might already know that this tangent line is perpendicular to the radius line touching that point. But let’s say you don’t already know that, or that you want a technique that generalizes to curves other than circles. As with other problems about slope of tangent lines, they key thought here is to zoom in close enough that the curve basically looks just like its own tangent line, then ask about a tiny step along that curve. The y-component of that little step is what you might call dy, and the x-component is a little dx, so the slope we’re looking for is the rise over run dy/dx. But unlike other tangent-slope problems in calculus, this curve is not the graph of a function, so we cannot take a simple derivative, asking about the size of a tiny nudge to the output of a function caused by some tiny nudge to the input. x is not an input and y is not an output in this case, they’re both just interdependent values related by some equation. This is called an “implicit curve”; it’s just the set of all points (x, y) that satisfy some property written in terms of the two variables x and y. The procedure for finding dy/dx here is what I found very weird as a calculus student, you take the derivative of both sides of this equation like this: For the derivative of x2 you write 2x*dx, similarly y2 becomes 2y*dy, and the derivative of the constant 52 on the right is 0. You can see why this feels strange, right? What does it mean to take a derivative of an expression with multiple variables? And why are we tacking on the little dy and dx in this way? But if you just blindly move forward with what you get here, you can rearrange to find an expression for dy/dx, which in this case comes out to -x/y. So at a point with coordinates (x, y) = (3, 4), that slope would be -¾, evidently. This strange process is called “implicit differentiation”. Don’t worry, I have an explanation for how you can interpret taking a derivative of an expression with two variables like this. But first, I want to set aside this particular problem, and show how this is related to a different type of calculus problem: Related rates. Imagine a 5 meter long ladder up against a wall, where the top of the ladder starts of 4 meters above the ground, which, by the pythagorean theorem, means the bottom is 3 meters away from the wall. And say it’s slipping down the wall in such a way that the top of the ladder is dropping at 1 meter per second. The question is, in that initial moment, what is the rate at which the bottom of the ladder is moving away from the wall. It’s interesting, right? That distance from the bottom of the ladder to the wall is 100% determined by the distance between the top of the ladder and the floor, so we should have enough information to figure out how the rates of change for each value depend on each other, but it might not be entirely clear at first how to relate the two. First thing’s first, it’s always nice to give names to the quantities we care about. So label the distance from the top of the ladder to the ground y(t), written as a function of time because it’s changing. Likewise, label the distance between the bottom of the ladder and the wall x(t). They key equation here that relates these terms is the pythagorean theorem: x(t)2 + y(t)2 = 52. What makes this equation powerful is that it’s true at all points in time. One way to solve this would be to isolate x(t), figure out what what y(t) must be based this 1 meter/second drop rate, then take a derivative of the resulting function; dx/dt, the rate at which x is changing with respect to time. And that’s fine; it involves a couple layers of using the chain rule, and it will definitely work for you. But I want to show a different way to think about the same thing. This left-hand side of the equation is a function of time, right? It just so happens to equal a constant, meaning this value evidently doesn’t change while time passes, but it’s still written as an expression dependent on time which we can manipulate like any other function with t as an input. In particular, we can take a derivative of the left hand side, which is a way of saying “If I let a little bit of time pass, dt, which causes y to slightly decrease, and x to slightly increase, how much does this expression change”. On the one hand, we know that derivative should be 0, since this expression equals a constant, and constants don’t care about your tiny nudge to time, they remain unchanged. But on the other hand, what do you get by computing the derivative of this left-hand-side? The derivative of x(t)2 is 2*x(t)*(the derivative of x). That’s the chain rule I talked about last video. 2x*dx represents the size of a change to x2 caused by a change to x, and we’re dividing by dt. Likewise, the rate at which y(t)2 is changing is 2*y(t)*(the derivative of y). Evidently, this whole expression must be zero, which is equivalent to saying x2+y2 doesn’t change while the ladder moves. And at the very start, t=0, the height y(t) is 4 meters, the distance x(t) is 3 meters, and since the top of the ladder is dropping at a rate of 1 meter per second, that derivative dy/dt is -1 meters/second. Now this gives us enough information to isolate the derivative dx/dt, which, when you work it out, is (4/3) meters per second. Now compare this to the problem of finding the slope of tangent line to the circle. In both cases, we had the equation x2 + y2 = 52, and in both cases we ended up taking the derivative of each side of this expression. But for the ladder problem, these expressions were functions of time, so taking the derivative has a clear meaning: it’s the rate at which this expression changes as time change. But what makes the circle situation strange is that rather than saying a small amount of time dt has passed, which causes x and y to change, the derivative has the tiny nudges dx and dy both just floating free, not tied to some other common variable like time. Let me show you how you can think about this: Give this expression x2 + y2 a name, maybe S. S is essentially a function of two variables, it takes every point (x, y) on the plane and associates it with a number. For points on this circle, that number is 25. If you step off that circle away from the center, that value would be bigger. For other points (x, y) closer to the origin, that value is smaller. What it means to take a derivative of this expression, a derivative of S, is to consider a tiny change to both these variables, some tiny change dx to x, and some tiny change dy to y –and not necessarily one that keeps you on this circle, by the way, it’s just some tiny step in any direction on the xy-plane– and ask how much the value of S changes. That difference in the value of S, from the original point to the nudged point, is what I’m writing as “dS”. For example, in this picture we’re starting at a point where x is 3 and y is 4, and let’s just say that step dx is... -0.02, and that dy is -0.01. Then the decrease to S, the amount the x2+y2 changes over that step, will be around 2(3)(-0.02) + 2(4)(-0.01). That’s what this derivative expression 2x*dx + 2y*dy means, it tells you how much the value x2+y2 changes, as determined by the point (x, y) where you started, and the tiny step (dx, dy) that you take. As with all things derivative, this is only an approximation, but it gets more and more true for smaller and smaller choices of dx and dy. The key point is that when you restrict yourself to steps along this circle, you’re essentially saying you want to ensure that this value S doesn’t change; it starts at a value of 25, and you want to keep it at a value of 25; that is, dS should be 0. So setting this expression 2x*dx + 2y*dy equal to 0 is the condition under which a tiny step stays on the circle. Again, this is only an approximation. Speaking more precisely, that condition keeps you on a tangent line of the circle, not the circle itself, but for tiny enough steps those are essentially the same thing. Of course, there’s nothing special about the expression x2+y2 = 52 here. You could have some other expression involving x’s and y’s, representing some other curve, and taking the derivative of both sides like this would give you a way to relate dx to dy for tiny steps along that curve. It’s always nice to think through more examples, so consider the expression sin(x)*y2 = x, which corresponds to many U-shaped curves on the plane. Those curves represent all the points (x, y) of the plane where the value of sin(x)*y2 equals the value of x. Now imagine taking some tiny step with components (dx, dy), and not necessarily one that keeps you on the curve. Taking the derivative of each side of this equation will tell us how much the value of that side changes during this step. On the left side, the product rule that we found in the last video tells us that this should be “left d-right plus right d-left”: sin(x)*(the change to y2), which is 2y*dy, plus y2*(the change to sin(x)), which is cos(x)*dx. The right side is simply x, so the size of a change to the value is exactly dx, right? Setting these two sides equal to each other is a way of saying “whatever your tiny step with coordinates (dx, dy) is, if it’s going to keep us on this curve, the values of both the left-hand side and the right-hand side must change by the same amount.” That’s the only way this top equation can remain true. From there, depending on what problem you’re solving, you could manipulate further with algebra, where perhaps the most common goal is to find dy divided by dx. As one more example, let me show how you can use this technique to help find new derivative formulas. I’ve mentioned in a footnote video that the derivative of ex is itself, but what about the derivative of its inverse function the natural log of x? The graph of ln(x) can be thought of as an implicit curve; all the points on the xy plane where y = ln(x), it just happens to be the case that the x’s and y’s of this equation aren’t as intermingled as they were in other examples. The slope of this graph, dy/dx, should be the derivative of ln(x), right? Well, to find that, first rearrange this equation y = ln(x) to be ey = x. This is exactly what the natural log of x means; it’s saying e to the what equals x. Since we know the derivative of ey, we can take the derivative of both sides, effectively asking how a tiny step with components (dx, dy) changes the value of each side. To ensure the step stays on the curve, the change to the left side of the equation, which is ey*dy, must equals the change to the right side, which is dx. Rearranging, this means dy/dx, the slope of our graph, equals 1/ey. And when we’re on this curve, ey is by definition the same as x, so evidently the slope is 1/x. An expression for the slope of the graph of function in terms of x like this is the derivative of that function, so evidently the derivative of ln(x) is 1/x. By the way, all of this is a little peek into multivariable calculus, where you consider functions with multiple inputs, and how they change as you tweak those inputs. The key, as always, is to have a clear image in your head of what tiny nudges are at play, and how exactly they depend on each other. Next up, I’ll talk about about what exactly a limit is, and how it’s used to formalize the idea of a derivative.
https://www.youtube.com/watch?v=kfF40MiS7zA 3Blue1Brown video   The last several videos have been about the idea of a derivative, and before moving on to integrals, I want to take some time to talk about limits. To be honest, the idea of a limit is not really anything new. If you know what the word “approach” means you pretty much already know what a limit is, you could say the rest is a matter of assigning fancy notation to the intuitive idea of one value getting closer to another. But there are actually a few reasons to devote a full video to this topic. For one thing it’s worth showing is how the way I’ve been describing derivatives so far lines up with the the formal definition of a derivative as it’s typically presented in most courses and textbooks. I want to give you some confidence that thinking of terms like dx and df as concrete non-zero nudges is not just some trick for building intuition; it’s actually backed up by the formal definition of a derivative in all its rigor. I also want to shed a little light on what exactly mathematicians mean by “approach”, in terms of something called the "epsilon delta" definition of limits. Then we’ll finish off with a clever trick for computing limits called L’Hopital’s rule. So first thing’s first, let’s take a look at the formal definition of the derivative. As a reminder, when you have some function f(x), to think about the derivative at a particular input, maybe x=2, you start by imagining nudging that input by some tiny dx, and looking at the resulting change to the output, df. The ratio df/dx, which can nicely be thought of as the rise-over-run slope between the starting point on the graph and the nudged point, is almost the derivative. The actual derivative is whatever this ratio approaches as dx approaches 0. Just to spell out what is meant here, that nudge to the output “df” is is the difference between f(starting-input + dx) and f(starting-input); the change to the output caused by the nudge dx. To express that you want to find what this ratio approaches as dx approaches 0, you write “l-i-m”, for limit, with “dx arrow 0” below it. Now, you’ll almost never see terms with a lowercase d, like dx, inside a limit like this. Instead the standard is to use a different variable, like delta-x, or commonly “h” for some reason. The way I like to think of it is that terms with this lowercase d in the typical derivative expression have built into them the idea of a limit, the idea that dx is supposed to eventually approach 0. So in a sense this lefthand side “df/dx”, the ratio we’ve been thinking about for the past few videos, is just shorthand for what the righthand side spells out in more detail, writing out exactly what we mean by df, and writing out the limiting process explicitly. And that righthand side is the formal definition of a derivative, as you’d commonly see it in any calculus textbook Now, if you’ll pardon me for a small rant here, I want to emphasize that nothing about this righthand side references the paradoxical idea of an “infinitely small” change. The point of limits is to avoid that. This value h is the exact same thing as the “dx” I’ve been referencing throughout the series. It’s a nudge to the input of f with some nonzero, finitely small size, like 0.001, it’s just that we’re analyzing what happens for arbitrarily small choices of h. In fact, the only reason people introduce a new variable name into this formal definition, rather than just using dx, is to be super-extra clear that these changes to the input are ordinary numbers that have nothing to do with the infinitesimal. You see, there are others who like to interpret dx as an “infinitely small change”, whatever that would mean, or to just say that dx and df are nothing more than symbols that shouldn’t be taken too seriously. But by now in the series, you know that I’m not really a fan of either of those views, I think you can and should interpret dx as a concrete, finitely small nudge, just so long as you remember to ask what happens as it approaches 0. For one thing, and I hope the past few videos have helped convince you of this, that helps to build a stronger intuition for where the rules of calculus actually come from. But it’s not just some trick for building intuitions. Everything I’ve been saying about derivatives with this concrete-finitely-small-nudge philosophy is just a translation of the formal definition of derivatives. Long story short, the big fuss about limits is that they let us avoid talking about infinitely small changes by instead asking what happens as the size of some change to our variable approaches 0. And that brings us to goal #2: Understanding exactly it means for one value to approach another. For example, consider the function [(2+h)3 - 23]/h. This happens to be the expression that pops out if you unravel the definition for the derivative of x3 at x=2, but let’s just think of it as any ol’ function with an input h. Its graph is this nice continuous looking parabola. But actually, if you think about what’s going at h=0, plugging that in you’d get 0/0, which is not defined. Just ask siri. So really, this graph has a hole at that point. You have to exaggerate to draw that hole, often with a little empty circle like this, but keep in mind the function is perfectly well-defined for inputs as close to 0 as you want. And wouldn’t you agree that as h approaches 0, the corresponding output, the height of this graph, approaches 12? And it doesn’t matter which side you come at it from. That the limit of this ratio as h goes to 0 equals 12. But imagine you’re a mathematician inventing calculus, and someone skeptically asks “well what exactly do you mean by approach?” That would be an annoying question. I mean, come on, we all know what it means for one value to get closer to another. But let me show you a way to answer completely unambiguously. For a given range of inputs within some distance of 0, excluding the forbidden point 0, look at the corresponding outputs, all possible heights of the graph above that range. As that range of input values closes in more and more tightly around 0, the range of output values closes in more and more closely around 12. The size of that range of outputs can be made as small as you want. As a counterexample, consider a function that looks like this, which is also not defined at 0, but kind of jumps at that point. As you approach h = 0 from the right, the function approaches 2, but as you come at 0 from the left, it approaches 1. Since there’s not a clear, unambiguous value that this function approaches as h approaches 0, the limit is simply not defined at that point. When you look at any range of inputs around 0, and the corresponding range of outputs, as you shrink that input range the corresponding outputs don’t narrow in on any specific value. Instead those outputs straddle a range that never even shrinks smaller than 1, no matter how small your input range. This perspective of shrinking an input range around the limiting point, and seeing whether or not you’re restricted in how much that shrinks the output range, leads to something called the “epsilon delta” definition of limits. You could argue this needlessly heavy-duty for an introduction to calculus. Like I said, if you know what the word “approach” means, you know what a limit means, so there’s nothing new on the conceptual level here. But this is an interesting glimpse into the field of real analysis, and it gives you a taste for how mathematicians made the intuitive ideas of calculus fully airtight and rigorous. You’ve already seen the main idea: when a limit exists, you can make this output range as small as you want; but when the limit doesn’t exist, that output range can’t get smaller than some value, no matter how much you shrink the input range around the limiting input. Phrasing that same idea a little more precisely, maybe in the context of this example where the limiting value was 12, think of any distance away from 12, where for some reason it’s common to use the greek letter “epsilon” to denote that distance. And the intent here is that that distance be something as small as you want. What it means for the limit to exist is that you can always find a range of inputs around our limiting input, some distance delta away from 0, so that any input within a distance delta of 0 corresponds to an output with a distance epsilon of 12. They key point is that this is true for any epsilon, no matter how small. In contrast, when a limit doesn’t exist, as in this example, you can find a sufficiently small epsilon, like 0.4, so that no matter how small you make your range around 0, no matter how tiny delta is, the corresponding range of outputs is just always too big. There is no limiting output value that they get arbitrarily close to. So far this is all pretty theory heavy; limits being used to formally define the derivative, then epsilons and deltas being used to rigorously define limits themselves. So let’s finish things off here with a trick for actually computing limits. For example, let’s say for some reason you were studying the function sin(pi*x)/(x2-1). Maybe this models some kind of dampened oscillation. When you plot a bunch of points to graph it, it looks pretty continuous, but there’s a problematic value, x=1. When you plug that in, sin(pi) is 0, and the denominator is also 0, so the function is actually not defined there, and the graph should really have a hole there. This also happens at -1, but let’s just focus our attention on one of these holes for now. The graph certainly does seem to approach some distinct value at that point, wouldn’t you say? So you might ask, how do you figure out what output this approaches as x approaches 1, since you can’t just plug in 1? Well, one way to approximate it would be to plug in a number very close to 1, like 1.00001. Doing that, you’d get a number around -1.57. But is there a way to know exactly what it is? Some systematic process to take an expression like this one, which looks like 0/0 at some input, and ask what its limit is as x approaches that input? Well, after limits so helpfully let us write the definition for a derivative, derivatives can come back to return the favor and help us evaluate limits. Let me show you what I mean. Here’s the graph of sin(pi*x), and here’s the graph of x2-1. That’s kind of a lot on screen, but just focus on what’s happening at x=1. The point here is that sin(pi*x) and x2-1 are both 0 at that point, so they cross the x-axis. In the same spirit as plugging in a specific value near 1, like 1.00001, let’s zoom in on that point and consider what happens a tiny nudge dx away. The value of sin(pi*x) is bumped down, and the value of that nudge, which was caused by the nudge dx to the input, is what we might call d(sin(pi*x)). From our knowledge of derivatives, using the chain rule, that should be around cos(pi*x)*pi*dx. Since the starting value was x=1, we plug in x=1 to this expression. In other words, the size of the change to this sin(pi*x) graph is roughly proportional to dx, with proportionality constant cos(pi)*pi. Since cos(pi) is exactly -1, we can write that as -pi*dx. Similarly, the value this x2-1 graph has changed by some d(x2-1). And taking the derivative, the size of that nudge should be 2*x*dx. Again, since we started at x=1, that means the size of this change is about 2*1*dx. So for values of x which are some tiny value dx away from 1, the ratio sin(pi*x)/(x2-1) is approximately (-pi*dx) / (2*dx). The dx’s cancel, so that value is -pi/2. Since these approximations get more and more accurate for smaller and smaller choices of dx, this ratio -pi/2 actually tells us the precise limiting value as x approaches 1. Remember, what that means is that the limiting height on our original graph is evidently exactly -pi/2. What happened there is a little subtle, so let me show it again, but this time a little more generally. Instead of these two specific functions, which both equal 0 at x=1, think of any two functions f(x) and g(x), which are both 0 at some common value x = a. And these have to be functions where you’re able to take a derivative of them at x = a, meaning they each basically look like a line when you zoom in close enough to that value. Even though you can’t compute f divided by g at the trouble point, since both equal zero, you can ask abou this ratio for values of x very close to a, the limit as x approach a. And it’s helpful to think of those nearby inputs as a tiny nudge dx away from a. The value of f at that nudged point is approximately its derivative, df/dx evaluated at a, times dx. Likewise the the value of g at that nudged point is approximately the derivative of g, evaluated at a, times dx. So near this trouble point, the ratio between the outputs of f and g is actually about the same as the derivative of f at a, times dx, divided by the derivative of g at a, times dx. These dx’s cancel, so the ratio of f and g near a is about the same as the ratio between their derivatives. Since those approximations get more accurate for smaller nudges, this ratio of derivatives gives the precise value for the limit. This is a really handy trick for computing a lot of limits. If you come across an expression that seems to equal 0/0 when you plug in some input, just take the derivative of the top and bottom expressions, and plug in that trouble input. This clever trick is called “L'Hôpital's rule”. Interestingly, it was actually discovered by Johann Bernoulli, but L’Hopital was a wealthy dude who essentially paid Bernoulli for the rights to some of his mathematical discoveries. In a very literal way, it pays to understand these tiny nudges. You might remember that the definition of a derivative for any given function comes down to computing the limit of a fraction that looks like 0/0, so you might think L’Hopital’s rule gives a handy way to discover new derivative formulas. But that would be cheating, since presumably you don’t yet know what the derivative on the numerator here is. When it comes to discovering derivative formulas, something we’ve been doing a fair amount this series, there is no systematic plug-and-chug method. But that’s a good thing. When creativity is required to solve problems like these, it’s a good sign you’re doing something real; something that might give you a powerful tool to solve future problems. Up next, I’ll talk about what an integral is, as well as the fundamental theorem of calculus, which is another example of where limits are used to help give a clear meaning to a fairly delicate idea that flirts with infinity. As you know, most support for this channel comes through Patreon, and the primary perk for patrons is early access to future series like this, where the next one will be on Probability. But for those of you who want a more tangible way to flag that you’re part of the community, there is also a small 3blue1brown store, links on the screen and in the description. I’m still debating whether or to make a preliminary batch of plushie pi creatures, it kind of depends on how many viewers seem interested in the store in general, but let me know in comments what kind of other things you’d like to see there.
https://www.youtube.com/watch?v=rfG8ce4nNh0 3Blue1Brown video   This guy, Grothendieck, is somewhat of a mathematical idol to me. And I just love this quote, don’t you? Too often in math we just dive into showing that a certain fact is true with long series of formulas before stepping back and making sure that it feels reasonable, and preferably obvious, at least on an intuitive level. In this video I want to talk about integrals, and the thing that I want to become “almost obvious” is that they are an inverse of derivatives. Here, we’ll focus just on one example, which is kind of dual to the example of a moving car that I talked about in chapter 2 of the series, introducing derivatives. Then in the next video, we’ll see how the idea generalizes into some other contexts. Imagine you’re sitting in a car, and you can’t see out the window; all you see is the speedometer. At some point, the car starts moving, speeds up, then slows back down to a stop, all over 8 seconds. The question is, is there a nice way to figure out how far you’ve traveled during that time, based only on your view of the speedometer? Or better yet, find a distance function s(t) that tells you how far you’ve traveled after any given amount of time, t, between 0 and 8 seconds. Let’s say you take note of the velocity at each second, and make a plot over time like this... And maybe you find that a nice function to model your velocity over time, in meters per second, is v(t) = t(8-t). You might remember, in chapter 2 of this series, we were looking at the opposite situation, where you know a distance function, s(t), and you want to figure out a velocity function from that. I showed how the derivative of your distance vs. time function gives you a velocity vs. time function, so in our current situation, where all we know is the velocity function, it should make sense that finding a distance vs. time function s(t) comes down to asking what function has a derivative t(8-t). This is often described as finding the anti-derivative of a function. And indeed, that’s what we’ll end up doing, and you could even pause and try that right now. But first, I want to spend the bulk of this video showing how this question is related to finding an area bounded by velocity graph, because that helps to build an intuition for a whole class of what are called “integral problems” in math and science. This question would be much simpler if the car was moving with a constant velocity, right? In that case, you could just multiply the velocity, in meters per second, by the amount of time passed, in seconds, and that gives you the number of meters traveled. Notice that you can visualize that distance as an area, and if visualizing distance as an area seems weird, I’m right there with you. It’s just that on this plot, where the horizontal direction has units of seconds and the vertical direction has units of meters/second, units of area very naturally correspond to meters. But what makes our situation hard is that the velocity not constant, it’s incessantly changing at every instant. It would even be a lot easier if it only ever changed at a handful of points, maybe staying static for the first second, then suddenly discontinuously jumping to a constant 7 meters per second for the next second, and so on, with discontinuous jumps to portions of constant velocity. That might make it very uncomfortable for the driver, in fact, it’s physically impossible, but it would make your calculations a lot more straightforward. You could compute the distance traveled on each interval by multiplying the constant velocity on that interval by the change in time. Then just add them all up. So what we’ll do is just approximate our velocity function as if it was constant on a bunch of different intervals. Then, as is common in calculus, we’ll see how refining that approximation leads us to something precise. Here, let’s make this more concrete with some numbers. Chop up the time axis between 0 and 8 into many small intervals, each with some little width dt, like 0.25 seconds. Consider one of these intervals, like the one between t=1, and 1.25. In reality the car speeds up from 7 m/s to about 8.4 m/s during that time, which you can find by plugging in t = 1 and 1.25 to the equation for velocity. We want to approximate the car’s motion as if its velocity was constant on this interval. Again, the reason for doing that is that we don’t really know how to handle anything other than a constant velocity situations. You could choose this constant to be anything between 7 and 8.4, it doesn’t really matter. All that matters is that that our sequence of approximations, whatever they are, gets better and better as dt gets smaller and smaller. That treating this car’s journey as a bunch of discontinuous jumps in speed between small portions of constant velocity becomes a less wrong reflection of reality as we decrease the time between those jumps. So for convenience, let’s just approximate the speed on each interval with whatever the true car’s velocity is at the start of the interval; the height of the graph above the left side, which in this case is 7. So on this example interval, according to our approximation, the car moves (7 m/s)*(0.25 s). That’s 1.75 meters, nicely visualized as the area of this thin rectangle. This is a little under the real distance traveled, but not by much. And the same goes for every other interval: The approximated distance is v(t)*dt, it’s just that you plug in a different value of t for each one, giving a different height for each rectangle. I’m going to write out an expression for the sum of the areas of all these rectangles in kind of a funny way. Take this symbol, which looks like a stretched “S” for sum, then put a 0 at its bottom and an 8 at its top to indicate that we’re ranging over time steps between 0 and 8 seconds. And as I said the amount we’re adding up at each time step is v(t)*dt. Two things are implicit in this notation: First, the value dt plays two roles: not only is it a factor in each quantity we’re adding up, it also indicates the spacing between each sampled time step. So when you make dt smaller, even though it decreases the area of each rectangle here, it increases the total number of rectangles whose areas we’re adding up. And second, the reason we don’t use the usual sigma notation to indicate a sum is that this expression is technically not any particular sum for any particular choice of dt; it’s whatever that sum approaches as dt approaches 0. As you can see, what that approaches is the area bounded by this curve and the horizontal axis. Remember, smaller choices of dt indicate closer approximations for our original question, how far does the car go, right? So this limiting value for the sum, the area under this curve, gives the precise answer to the question, in full unapproximated precision. Now tell me that’s not surprising. We have this pretty complicated idea of approximations that can involve adding up a huge number of very tiny things, and yet the value those approximates approach can be described so simply, as the area under a curve. This expression is called an “integral” of v(t), since it brings all of its values together, it integrates them. Now you could say, “How does this help!?. You’ve just reframed one hard question, finding how far the car has traveled, into another equally hard problem, finding the area between this graph and the horizontal axis?” And...you’d be right! If the velocity/distance duo was all we cared about, most of this video with all this area under a curve nonsense would be a waste of time. We could just skip straight ahead to figuring out an antiderivative. But finding the area between a function’s graph and the horizontal axis is somewhat of a common language for many disparate problems that can be broken down and approximated as the sum of a large number of small things. You’ll see more next video, but for now I’ll just say in the abstract that understanding how interpret and compute the area under a graph is a very general problem-solving tool. In fact, the first video of this series already covered the basics of how this works, but now that we have more of a background with derivatives, we can actually take the idea to its completion. For our velocity example, think of this right endpoint as a variable, capital T. So we’re thinking of this integral of the velocity function between 0 and T, the area under this curve between those two inputs, as a function, where that upper bound is the variable. That area represents the distance the car has traveled after T seconds, right? So this is really a distance vs. time function s(T). Now ask yourself: What is the derivative of that function? On the one hand, a tiny change in distance over a tiny change in time is velocity; that’s what velocity means. But there’s another way to see it purely in terms of this graph and this area, which generalizes better to other integral problems. A slight nudge of dT to the input causes that area to increase, some little ds represented by the area of this sliver. The height of that sliver is the height of the graph at that point, v(T), and its width is dT. And for small enough dT, we can basically consider that sliver to be a rectangle. So the area of that sliver, ds, is approximately equal to v(T)*dT. Because this approximation gets better and better for smaller dT, the derivative of the area function ds/dT at this point equals v(T), the value of the velocity function at whatever time we started on. And that’s super general, the derivative of any function giving the area under a graph like this is equal to the function for the graph itself. So if our velocity function here is t*(8-t), what should s be? What function of t has a derivative t*(8-t). This is where we actually have to roll up our sleeves and do some math. It’s easier to see if we expand this out as 8t - t2. Take each part here one at a time: What function has a derivative 8t? Well, we know that the derivative of t2 is 2t, so if we just scale that up by 4, we see that the derivative of 4t2 is 8t. And for that second part, what kind of function might have -t2 as its derivative? Using the power rule again, we know that the derivative of a cubic term, t3, gives a squared term, 3t2, so if we scale that down by a third, the derivative of (⅓)t3 is exactly t2, and making that negative we see that -(⅓)t3 has a derivative of -t2. Therefore, the antiderivative of 8t - t2 is 4t2 - (⅓)t3. But there’s a slight issue here: we could add any constant to this function, and its derivative would still be 8t - t2. The derivative of a constant is always 0. And if we graph s(t), you can think of this in the sense that moving a graph of a distance function up and down does nothing to affect its slope above each input. So there are actually infinitely many different possible antiderivative functions, all of which look like 4t2 - (⅓)t3 + C for some constant C. But there is one piece of information we haven’t used yet that let’s us zero in on which antiderivative to use: The lower bound on the integral. This integral must be zero when we drag that right endpoint all the way to the left endpoint, right? The distance traveled by the car between 0 seconds and 0 seconds is...zero. So as we found, this area as a function of capital T is an antiderivative for the stuff inside, and to choose what constant to add, subtract off the value of that antiderivative function at the lower bound. If you think about it for a moment, that ensures that the integral from the lower bound to itself will indeed be 0. As it so happens, when you evaluate the function we have here at t=0, you get zero, so in this specific case you don’t actually need to subtract off anything. For example, the total distance traveled during the 8 seconds is this expression evaluated at T=8, which is 85.33, minus 0. But a more typical example would be something like this integral between 1 and 7. That’s the area pictured here, and it represents the distance traveled between 1 second and 7 seconds. What you’d do is evaluate the antiderivative we found at the top bound, 7, and subtract off its value at the bottom bound, 1. Notice, it doesn’t matter what antiderivative we choose here; if for some reason it had a constant added to it, like 5, that constant would cancel out. More generally, anytime you want to integrate some function –and remember you think of as adding up the values f(x)*dx for inputs in a certain range then asking what that sum approaches as dx approaches 0– the first step is to find an antiderivative, some other function, “capital F(x)”, whose derivative is the thing inside the integral. Then the integral equals this antiderivative evaluated at the top bound, minus its value at the bottom bound. This fact is called the “fundamental theorem of calculus”. Here’s what’s crazy about this fact: The integral, the limiting value for the sum of all these thin rectangles, takes into account every single input on the continuum from the lower bound to the upper bound, that’s why we use the word “integrate”; it brings them together. And yet, to actually compute it using the antiderivative, you look at only two inputs: the top and the bottom. It almost feels like cheating! Finding the antiderivative implicitly accounts for all the information needed to add up all the values between the lower bound and upper bound. There’s kind of a lot packed into this whole concept, so let’s recap everything that just happened, shall we? We wanted to figure out how far a car goes just by looking at the speedometer, and what makes that hard is that the velocity was always changing. If you approximate it to be constant on multiple different intervals, you can figure out how far the car goes on each interval just with multiplication, then add them all up. Adding up those products can be visualized as the sum of the areas of many thin rectangles like this. Better and better approximations of the original problem correspond to collections of rectangles whose aggregate area is closer and closer to being the area under this curve between the start time and end time, so that area under the curve is the precise distance traveled for the true, nowhere-constant velocity function. If you think of this area as function, with a variable right end point, you can deduce that the derivative of that area function must equal the height of the graph at each point. That’s the key! So to find a function giving this area, you ask what function has v(t) as its derivative. There are actually infinitely many antiderivatives of a given function, since you can always just add some constant without affecting the derivative, so you account for that by subtracting off the value of whatever antiderivative function you choose at the bottom bound. By the way, one important thing to bring up before leaving is the idea of negative area. What if our velocity function was negative at some point? Meaning the car is going backwards. It’s still true that the tiny distance traveled ds on a little time interval is about equal to the velocity times the tiny change in time, it’s just that that the number you’d plug in for velocity would be negative, so that tiny change in distance is negative. In terms of our thin rectangles, if the rectangle goes below the horizontal axis like this, its area represents a bit of distance traveled backwards, so if what you want is to find the distance between the car’s start point and end point, you’d want to subtract it. And this is generally true of integrals: Whenever a graph dips below the horizontal axis, that area underneath is counted as negative. What you’ll commonly hear is that integrals measure the “signed” area between a graph and the horizontal axis. Next up I’ll bring up more contexts where this idea of an integral and the area under curves comes up, along with some other intuitions for the fundamental theorem of calculus. Perhaps you remember, chapter 2 of this series, introducing the derivative was sponsored by the Art of Problem Solving. So I think there’s something elegant to the fact that this video, which is kind of a dual to that one, was also supported in part by the Art of Problem Solving. I really can’t imagine a better sponsor for the channel, because it’s a company whose books and courses I recommend to people anyway. They were highly influential to me, when I was a student developing a love for creative math, so if you’re a parent looking to foster your own child’s love for the subject, or if you’re a student who wants to see what math has to offer beyond rote school work, I cannot recommend the Art of Problem Solving enough. Whether that’s their newest development to build the right intuitions in elementary schools kids, called Beast academy, or their courses on higher level topics and contest preparation. Going to AoPS.com/3blue1brown, or clicking the link on the screen, lets them know you came from this channel, which may encourage them to support future projects like this one. I consider these videos a success not when they teach people a particular bit of math, which can only ever be a drop in the ocean, but when they encourage people to go explore the expanse of math for themselves. And the Art of Problem Solving is among the few great places to actually do that.
https://www.youtube.com/watch?v=FnJqaIESC2s 3Blue1Brown video   Here, I want to discuss one common type of problem where integration comes up: Finding the average of a continuous variable. This is a useful thing to know in its own right, but what’s really neat is that it gives a completely different perspective for why integrals and derivatives are inverses of one and other. Take a look at the graph of sin(x) between 0 and pi, which is half its period. What is the average height of this graph on that interval? It’s not a useless question. All sorts of cyclic phenomena in the world are modeled with sine waves: For example, the number of hours the sun is up per day as a function of which-day-of-the-year-it-is follows a sine wave pattern. So if you wanted to predict, say, the average effectiveness of solar panels in summer months vs. winter months, you’d want to be able to answer a question like this: What’s the average value of that sine function over half its period. Whereas a case like that will have all sorts of constants mucking up the function, we’ll just focus on a pure unencumbered sin(x) function, but the substance of the approach would be the same in any application. It’s kind of a weird thing to think about, isn’t it, the average of a continuous variable. Usually, with averages, we think of a finite number of values, where you add all them up and divide that sum by how many values there are. But there are infinitely many values of sin(x) between 0 and pi, and its not like we can add all those numbers and divide by infinity. This sensation actually comes up a lot in math, and is worth remembering, where you have this vague sense that you want to add together infinitely many values associated with a continuum like this, even though that doesn’t really make sense. Almost always, when you get this sense, the key will be to use an integral somehow. And to think through exactly how, a good first step is usually to approximate your situation with some kind of finite sum. In this case, imagine sampling a finite number of points, evenly spaced in this range. Since it’s a finite sample, you can find the average by adding up all the heights, sin(x), at each one, and divide that sum by the number of points you sampled, right? And presumably, if the idea of an average height among all infinitely many points is going to make any sense at all, the more points we sample, which would involve adding up more heights, the closer the average of that sample should be to the actual average of the continuous variable, don’t you think? This should feel at least somewhat related to taking an integral of sin(x) between 0 and pi, even if it might not be clear at first exactly how the two ideas will match up. For that integral, you also think of a sample of inputs on this continuum, but instead of adding the height sin(x) at each one, and dividing by how many there are, you add up sin(x)*dx where dx is the spacing between the samples; that is, you’re adding little areas, not heights. Technically, the integral is not quite this sum, it’s whatever that sum approaches as dx approaches 0. But it’s helpful to reason with respect to one of these finite iterations, where you’re adding the areas of some specific number of rectangles. So what you want to do is reframe this expression for the average, this sum of the heights divided by the number of sampled points, in terms of dx, the spacing between samples. If I tell you that the spacing between these points is 0.1, for example, and you know that they range from 0 to pi, can you tell me how many there are? Well, you can take that length of the interval, pi, and divide it by the length of the space between each sample. If it doesn’t go in evenly, you’d round down to the nearest integer, but as an approximation this is fine. So if we write the spacing between samples as dx, the number of samples is pi/dx. So replacing the denominator with pi/dx here, you can rearrange, putting the dx up top and distributing. But, think about what it means to distribute that dx up top; it means the terms you’re adding all look like sin(x)*dx for the various inputs x that you’re sampling, so that numerator looks exactly like an integral expression. And for larger and larger samples of points, the average approaches the actual integral of sin(x) between 0 and pi, all divided by the length of that range, pi. In other words, the average height of this graph is this area divided by its width. On an intuitive level, and just thinking in terms of units, that feels pretty reasonable, doesn’t it? Area divided by with gives you average height. So let’s actually compute this expression. As we saw, last video, to compute an integral you need to find an antiderivative of the function inside the integral; some function whose derivative is sin(x) And, if you’re comfortable with trig derivatives, you know the derivative of cos(x) is -sin(x), so if you negate that, -cos(x) is the antiderivative of sin(x). To gut check yourself on that, look at this graph of -cos(x). At 0, the slope is 0, then it increases to some maximum slope at pi/2, then it goes back down to 0 at pi, and in general its slope does indeed seem to match the height of the sine graph. To evaluate the integral of sin(x) between 0 and pi, take that antiderivative at the upper bound, and subtract its value at the lower bound. More visually, that’s the difference in the height of this -cos(x) graph above pi, and its height at 0, and as you can see, that change in height is exactly 2. That’s kind of interesting, isn’t it? That the area under this sine graph turns out to be exactly 2. So the answer to our average height problem, this integral divided by the width of the region, evidently turns out to be 2/pi, which is around 0.64. I promised at the start that this question of finding the average value of a function offers an alternate perspective on on why integrals and derivatives are inverses of one and other; why the area under one graph is related to the slope of another. Notice how finding this average value 2/pi came down to looking at the change in the antiderivative -cos(x) over the input range, divided by the length of that input range. Another way to think about that fraction is as the rise-over-run slope between the point of the antiderivative graph below zero, and the point of that graph above pi. Now think about why it might make sense that this slope represents the average value of sin(x) on that region. By definition, sin(x) is the derivative of this antiderivative graph; it gives the slope of -cos(x) at every input. So another way to think about the average value sin(x) is as the average slope over all tangent lines here between 0 and pi. And from that view, doesn’t it make a lot of sense that the average slope of a graph over all its point in a certain range should equal the total slope between the start and end point? To digest this idea, it helps to see what it looks like for a general function. For any function f(x), if you want to find its average value on some interval, say between a and b, what you do is take the integral of f on that interval, divided by the width of the interval. You can think of this as taking the area under the graph divided by the width. Or more accurately, it’s the signed area of that graph, since area below the x-axis is counted as negative. And take a moment to remember the connection between this idea of a continuous average and the usual finite notion of an average, where you add up many numbers and divide by how many there are. When you take some sample of points spaced out by dx, the number of samples is about the length of the interval divided by dx. So if you add up the value of f(x) at each sample and divide by the total number of samples, it’s the same as adding up the products f(x)*dx and dividing by the width of the entire interval. The only difference between that and the integral expression is that the integral asks what happens as dx approaches 0, but that just corresponds with samples of more and more points that approximate the true average increasingly well. Like any integral, evaluating this comes down to finding an antiderivative of f(x), commonly denoted capital F(x). In particular, what we want is the change to this antiderivative between a and b, F(b) - F(a), which you can think of as the change in the height of this new graph between the two bounds. I’ve conveniently chosen an antiderivative which passes through 0 at the lower bound here, but keep in mind that you could freely shift this up and down, adding whatever constant you want to it, and it would still be a valid antiderivative. So the solution to the average problem is the change in the height of this new graph divided by the change to its x value between a and b. In other words, it’s the slope of the antiderivative graph between these endpoints. And again, that should make a lot of sense, because little f(x) gives the slope of a tangent line to this graph at each point, after all it is by definition the derivative of capital F. So, why are antiderivative the key to solving integrals? Well, my favorite intuition is still the one I showed last video, but a second perspective is that when you reframe the question of finding the average of a continuous value as finding the average slope of bunch of tangent lines, it lets you see the answer just by the comparing endpoints, rather than having to actually tally up all points in between. In the last video, I described a sensation that should bring integrals to your mind. Namely, if you feel like the problem you’re solving could be approximated by breaking it up somehow, and adding up a large number of small things. Here I want you to come away recognizing a second sensation that should bring integrals to your mind. If there’s some idea that you understand in a finite context, and which involves adding up multiple values, like taking the average of a bunch of numbers, and if you want to generalize that idea to apply to an infinite continuous range of values, try seeing if you can phrase things in terms of an integral. It’s a feeling that comes up enough that’s it’s definitely worth remembering. My thanks, as always, to those making these videos possible.
https://www.youtube.com/watch?v=BLkz5LGWihw 3Blue1Brown video   In the next chapter, about Taylor series, I make frequent reference to higher order derivatives. And, if you’re already comfortable with second derivatives, third derivatives and such, great! Feel free to skip right ahead to the main event now, you won’t hurt my feelings. But somehow I’ve managed not to bring up higher order derivatives at all so far this series, so for the sake of completeness, I thought I’d give this little footnote to very briefly go over them. I’ll focus mainly on the second derivative, showing what it looks like in the context of graphs and motion, and leave you to think about the analogies for higher orders. Given some function f(x), the derivative can be interpreted as the slope of its graph above some input, right? A steep slope means a high value for the derivative, a downward slope means a negative derivative. The second derivative, whose notation I’ll explain in a moment, is the derivative of the derivative, meaning it tells you how that slope is changing. The way to see this at a glance is to think of how the graph of f(x) curves. At points where it curves upward, the slope is increasing, so the second derivative is positive. At points where it curves downward, the slope is decreasing, so the second derivative is negative. For example, a graph like this has a very positive second derivative at the input 4, since the slope is rapidly increasing around that point, whereas a graph like this still has a positive second derivative at that same point, but it’s smaller, since the slope is increasing only slowly. At points where there’s not really any curvature, the second derivative is zero. As far as notation goes, you could try writing it like this, indicating some small change to the derivative function divided by some small change to x, where as always the use of that letter d suggests that you really want to consider what this ratio approach as dx, both dx’s in this case, approach 0. That’s pretty awkward and clunky, so the standard is to abbreviate it as d2f/dx2. It’s not terribly important for getting an intuition of the second derivative, but perhaps it’s worth showing how you can read this notation. Think of starting at some input to your function, and taking two small steps to the right, each with a size dx. I’m choosing rather big steps here so that we’ll better see what’s going on, but in principle think of them as rather tiny. The first step causes some change to the function, which I’ll call df1, and the second step causes some similar, but possibly slightly different change, which I’ll call df2. The difference between these; the change in how the function changes, is what we’ll call d(df). You should think of this as really small, typically proportional to the size of (dx)2. So if your choice for dx was 0.01, you’d expect this d(df) to be proportional to 0.001. And the second derivative is the size of this change to the change, divide by the size of (dx)2. Or, more precisely, it’s whatever that ratio approaches as dx approaches 0. Even though it’s not like the letter d is a variable being multiplied by f, for the sake of more compact notation you write this as d2f/dx2, and you don’t bother with any parentheses on the bottom. Maybe the most visceral understanding of the second derivative is that it represents acceleration. Given some movement along a line, suppose you have some function that records distance traveled vs. time, and maybe its graph looks something like this, steadily increasing over time. Then its derivative tells you velocity at each point in time, right? For the example, the graph might look like this bump, increasing to some maximum, then decreasing back to 0. So its second derivative tells you the rate of change for velocity, the acceleration at each point in time. In the example, the second derivative is positive for the first half of the journey, which indicates indicates speeding up. That’s sensation of being pushed back into your car seat with a constant force. Or rather, having the car seat push you with a constant force. A negative second derivative indicates slowing down, negative acceleration. The third derivative, and this is not a joke, is called jerk. So if the jerk is not zero, it means the strength of the acceleration itself is changing. One of the most useful things about higher order derivatives is how they help in approximating functions, which is the topic of the next chapter on Taylor series, so I’ll see you there.